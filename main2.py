# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kaRWtk2S5BstuEkC7iTCG9lz-Nisl4R2
"""

import tensorflow as tf
import numpy as np
import os
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Reshape
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

def create_dataset(data_dir, subset='train'):
    data_path = os.path.join(data_dir, subset)
    patches_list = []
    labels = []
    users = sorted(os.listdir(data_path))
    user_to_idx = {user: idx for idx, user in enumerate(users)}

    for user in users:
        user_path = os.path.join(data_path, user)
        image_folders = [f for f in os.listdir(user_path) if f.startswith('image')]

        for img_folder in image_folders:
            folder_path = os.path.join(user_path, img_folder)
            patches = []

            patch_files = sorted(os.listdir(folder_path))[:500]  # Ensure exactly 500 patches
            for patch_file in patch_files:
                patch_path = os.path.join(folder_path, patch_file)
                patch = np.load(patch_path)  # Shape: (16, 16, 8)
                patches.append(patch)

            patches_array = np.stack(patches)  # Shape: (500, 16, 16, 8)
            patches_list.append(patches_array)
            labels.append(user_to_idx[user])

    return np.array(patches_list), np.array(labels), user_to_idx

def create_model(num_users):
    inputs = Input(shape=(500, 16, 16, 8))

    # Normalize inputs
    normalized_inputs = BatchNormalization()(inputs)

    # Create a deeper CNN for processing each patch
    patch_processor = tf.keras.Sequential([
        Conv2D(32, (5, 5), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.3),
        Conv2D(64, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.3),
        Conv2D(128, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Dropout(0.4),
        Conv2D(256, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        GlobalAveragePooling2D(),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.5)
    ])

    # Process each patch using TimeDistributed
    processed_patches = tf.keras.layers.TimeDistributed(patch_processor)(normalized_inputs)

    # Flatten all processed patches
    x = Reshape((-1,))(processed_patches)  # Shape: (batch_size, 500 * 128)

    # Final classification layers
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    outputs = Dense(num_users, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)

    # Compile model with lower learning rate
    model.compile(
        optimizer=Adam(learning_rate=1e-3),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

def normalize_patches(patches):
    mean = np.mean(patches, axis=(1, 2, 3, 4), keepdims=True)
    std = np.std(patches, axis=(1, 2, 3, 4), keepdims=True)
    return (patches - mean) / (std + 1e-8)

def train_model(data_dir, batch_size=32, epochs=100):
    # Load and prepare training data
    X_train, y_train, user_to_idx = create_dataset(data_dir, 'train')
    X_train = normalize_patches(X_train)

    # Load and prepare test data
    X_test, y_test, _ = create_dataset(data_dir, 'test')
    X_test = normalize_patches(X_test)

    # Create model
    num_users = len(user_to_idx)
    model = create_model(num_users)

    # Split training data into train and validation
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )

    # Create tf.data.Dataset for efficient loading
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\
        .shuffle(buffer_size=1000)\
        .batch(batch_size)\
        .prefetch(tf.data.AUTOTUNE)

    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\
        .batch(batch_size)\
        .prefetch(tf.data.AUTOTUNE)

    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\
        .batch(batch_size)\
        .prefetch(tf.data.AUTOTUNE)

    # Callbacks
    callbacks = [
        ModelCheckpoint(
            'best_hand_model.h5',
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        )
    ]

    # Train model
    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=epochs,
        callbacks=callbacks,
        verbose=1
    )

    # Evaluate on test set
    test_loss, test_accuracy = model.evaluate(test_dataset, verbose=1)
    print(f"\nTest accuracy: {test_accuracy:.4f}")

    return model, history, user_to_idx, test_accuracy

def predict_user(model, patches, user_to_idx):
    patches = patches.reshape(1, 500, 16, 16, 8)  # Add batch dimension
    patches = normalize_patches(patches)

    predictions = model.predict(patches)[0]
    predicted_idx = np.argmax(predictions)
    confidence = predictions[predicted_idx]

    idx_to_user = {idx: user for user, idx in user_to_idx.items()}
    predicted_user = idx_to_user[predicted_idx]

    return predicted_user, confidence

if __name__ == "__main__":
    # Train the model
    model, history, user_to_idx, test_accuracy = train_model(
        data_dir='data_final',
        batch_size=32,  # Increased batch size
        epochs=100      # Increased training epochs
    )
    print(f"Final test accuracy: {test_accuracy:.4f}")