# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kaRWtk2S5BstuEkC7iTCG9lz-Nisl4R2
"""

import tensorflow as tf
import numpy as np
import os
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input, Reshape, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

def create_dataset(data_dir, subset='train'):
    """
    Creates dataset from the directory structure
    Args:
        data_dir: Root directory containing user folders
        subset: 'train' or 'test'
    Returns:
        patches_list: List of patch arrays
        labels: List of corresponding labels
        user_to_idx: Dictionary mapping usernames to indices
    """
    data_path = os.path.join(data_dir, subset)
    patches_list = []
    labels = []
    users = sorted(os.listdir(data_path))
    user_to_idx = {user: idx for idx, user in enumerate(users)}

    for user in users:
        user_path = os.path.join(data_path, user)
        image_folders = [f for f in os.listdir(user_path) if f.startswith('image')]

        for img_folder in image_folders:
            folder_path = os.path.join(user_path, img_folder)
            patches = []

            # Load all 500 patches from the folder
            patch_files = sorted(os.listdir(folder_path))[:500]  # Ensure exactly 500 patches
            for patch_file in patch_files:
                patch_path = os.path.join(folder_path, patch_file)
                patch = np.load(patch_path)  # Shape: (16, 16, 8)
                patches.append(patch)

            patches_array = np.stack(patches)  # Shape: (500, 16, 16, 8)
            patches_list.append(patches_array)
            labels.append(user_to_idx[user])

    return np.array(patches_list), np.array(labels), user_to_idx

def create_model(num_users):
    """
    Creates the CNN model for spectral patch processing
    Args:
        num_users: Number of users to classify
    Returns:
        tf.keras.Model: Compiled model
    """
    # Input shape: (500, 16, 16, 8) - 500 patches per sample, each 16x16 with 8 spectral bands
    inputs = Input(shape=(500, 16, 16, 8))

    # Create a CNN to process each patch
    patch_processor = tf.keras.Sequential([
        Conv2D(32, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Conv2D(64, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D((2, 2)),
        Conv2D(128, (3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        GlobalAveragePooling2D(),
        Dense(64, activation='relu'),
        BatchNormalization()
    ])

    # Process each patch using TimeDistributed
    processed_patches = tf.keras.layers.TimeDistributed(patch_processor)(inputs)

    # Flatten all processed patches
    x = Reshape((-1,))(processed_patches)  # Shape: (batch_size, 500 * 64)

    # Final classification layers
    x = Dense(512, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.3)(x)
    outputs = Dense(num_users, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)

    # Compile model
    model.compile(
        optimizer=Adam(learning_rate=1e-6),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

def normalize_patches(patches):
    """
    Normalize the patches data
    Args:
        patches: Array of shape (num_samples, 500, 16, 16, 8)
    Returns:
        Normalized patches
    """
    # Normalize across all dimensions except the batch dimension
    mean = np.mean(patches, axis=(1, 2, 3, 4), keepdims=True)
    std = np.std(patches, axis=(1, 2, 3, 4), keepdims=True)
    return (patches - mean) / (std + 1e-8)

def train_model(data_dir, batch_size=16, epochs=50):
    """
    Main training function
    Args:
        data_dir: Root directory containing train/test folders
        batch_size: Batch size for training
        epochs: Number of epochs to train
    """
    # Load and prepare training data
    X_train, y_train, user_to_idx = create_dataset(data_dir, 'train')
    X_train = normalize_patches(X_train)

    # Load and prepare test data
    X_test, y_test, _ = create_dataset(data_dir, 'test')
    X_test = normalize_patches(X_test)

    # Create model
    num_users = len(user_to_idx)
    model = create_model(num_users)

    # Split training data into train and validation
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.1, random_state=42, stratify=y_train
    )

    # Create tf.data.Dataset for efficient loading
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\
        .shuffle(buffer_size=1000)\
        .batch(batch_size)\
        .prefetch(tf.data.AUTOTUNE)

    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\
        .batch(batch_size)\
        .prefetch(tf.data.AUTOTUNE)

    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\
        .batch(batch_size)\
        .prefetch(tf.data.AUTOTUNE)

    # Callbacks
    callbacks = [
        ModelCheckpoint(
            'best_hand_model.h5',
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        ),
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        )
    ]

    # Train model
    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=epochs,
        callbacks=callbacks,
        verbose=1
    )

    # Evaluate on test set
    test_loss, test_accuracy = model.evaluate(test_dataset, verbose=1)
    print(f"\nTest accuracy: {test_accuracy:.4f}")

    return model, history, user_to_idx, test_accuracy

def predict_user(model, patches, user_to_idx):
    """
    Predict user for new patches
    Args:
        model: Trained model
        patches: Array of shape (500, 16, 16, 8) containing patches
        user_to_idx: Dictionary mapping usernames to indices
    Returns:
        Predicted username and confidence
    """
    # Normalize patches
    patches = patches.reshape(1, 500, 16, 16, 8)  # Add batch dimension
    patches = normalize_patches(patches)

    # Get predictions
    predictions = model.predict(patches)[0]
    predicted_idx = np.argmax(predictions)
    confidence = predictions[predicted_idx]

    # Get username from index
    idx_to_user = {idx: user for user, idx in user_to_idx.items()}
    predicted_user = idx_to_user[predicted_idx]

    return predicted_user, confidence

if __name__ == "__main__":
    # Train the model
    model, history, user_to_idx, test_accuracy = train_model(
        data_dir='data_final',
        batch_size=16,
        epochs=50
    )
    print(f"Final test accuracy: {test_accuracy:.4f}")